# These rules are part of Acanthophis. See https://github.com/kdm9/Acanthophis.
# This file *could* be modified, but then be careful when you update them. And
# please, if you find a bug, raise an issue on github so the fix gets shared
# with everyone.
#
# Copyright 2016-2024 Kevin Murray/Gekkonid Consulting
#
# This Source Code Form is subject to the terms of the Mozilla Public License,
# v. 2.0. If a copy of the MPL was not distributed with this file, You can
# obtain one at http://mozilla.org/MPL/2.0/.

#######################################################################
#                         Helper python code                          #
#######################################################################
#
# Traditionally, this lived in `utils/snkmk.py` or in `import acanthophis`, but
# because of the way snakemake sends execution context to the k8s controller,
# many local files aren't copied. This include any `$module.py` files lying
# alongside and `import $module`'d in rules files. So, I have copied the
# relevant bits from both Norman's original pipeline and Acanthopis to
# `base.rules`, which is sourced first in the Snakefile. This means that
# `import $module` (i.e. `import acanthophis` or `import snkmk`) is now
# `include: "rules/base.rules"`, and you don't use the module name to access
# any function (i.e. it's like `from $module import *`). Obviously don't use
# any of these functions before doing `include: "rules/base.rules"` or the
# functions won't be defined. 

import csv
from collections import defaultdict
from copy import deepcopy
from glob import glob
from os.path import basename, splitext, dirname
import os
from sys import stderr
from math import log, inf
from functools import partial
import pathlib
from natsort import natsorted


try:
    from ._version import version
    __version__ = version
except ImportError:
    pass


def populate_metadata(config, runlib2samp=None, sample_meta=None, setfile_glob=None):
    try:
        if runlib2samp is None:
            runlib2samp = config["data_paths"]["metadata"]["runlib2samp_file"]
        if setfile_glob is None:
            setfile_glob = config["data_paths"]["metadata"]["setfile_glob"]
    except KeyError as exc:
        raise ValueError("ERROR: metadata files must be configured in config, or passed to populate_metadata()")

    rl2s_meta = parse_metadata(runlib2samp)
    meta_by_rl = {}
    for rl in rl2s_meta:
        meta_by_rl[(rl["run"], rl["library"])] = rl

    RL2S, S2RL = make_runlib2samp(rl2s_meta)
    config["RUNLIB2SAMP"] = RL2S
    config["SAMP2RUNLIB"] = S2RL
    config["SAMPLESETS"] = make_samplesets(config, runlib2samp, setfile_glob)
    config["RAWDATA_PATHS"] = {}
    for rl, meta in meta_by_rl.items():
        r1 = meta.get("read1_uri", "")
        r2 = meta.get("read2_uri", "")
        il = meta.get("interleaved_uri", "")
        rs = meta.get("single_uri", "")
        ret = {}
        if r1 and r2:
            ret["r12"] = {"r1": r1, "r2": r2}
        if rs:
            ret["se"] = rs
        if r1 and not r2:
            print(f"WARNING: {r1} comes without an R2 read, treating it as SE", file=stderr)
            ret["se"] = r1
        if r2 and not r1:
            raise ValueError(f"R1 path given without R2 or vice versa for Run/Library {rl}")
        if il:
            ret["il"] = il
        if ret == {}:
            raise ValueError(f"No data paths given for Run/Library {rl}")
        config["RAWDATA_PATHS"][rl] = ret


    # In the rl2s we have a column for "qc_type". in the config file, we
    # have a set of adaptorremoval (and potentially other settings) keyed
    # by qc_type, with a sensible default as __default__. So here, we map
    # each runlib to the qc type, which we later query for a given tool in the
    # rules file (in a params: block).
    config["RL_QCTYPE"]= {
        rl: kv.get("qc_type", "__default__")
        for rl, kv in meta_by_rl.items()
    }


def parse_metadata(s2rl_file):
    meta = []
    with open(s2rl_file) as fh:
        dialect = "excel"
        if s2rl_file.endswith(".tsv"):
            dialect = "excel-tab"
        for run in csv.DictReader(fh, dialect=dialect):
            if run.get("include", "Y").upper() != "Y" or run.get("exclude", "N").upper() == "Y":
                # Remove non-sequenced ones
                continue
            if run.get("exclude_why", ""):
                continue
            meta.append({k.lower(): v for k, v in run.items()})
    return meta


def make_runlib2samp(rl2s_meta):
    rl2s = {}
    s2rl = defaultdict(list)
    for run in rl2s_meta:
        rl = (run["run"], run["library"])
        samp = run["sample"]
        rl2s[rl] = samp
        s2rl[samp].append(rl)
    print(f"Parsed {len(rl2s)} run-libs from {len(s2rl)} samples")
    return dict(rl2s), dict(s2rl)


def stripext(path, exts=".txt"):
    if isinstance(exts, str):
        exts = [exts,]
    for ext in exts:
        if path.endswith(ext):
            path = path[:-len(ext)]
    return path


def make_samplesets(config, s2rl_file, setfile_glob):
    ssets = defaultdict(list)
    everything = set(config.get("SAMP2RUNLIB", {}).keys())
    for setfile in glob(setfile_glob):
        setname = stripext(basename(setfile), ".txt")
        with open(setfile) as fh:
            samples = [x.strip() for x in fh]
        for samp in samples:
            if samp not in everything:
                raise ValueError(f"ERROR: sample '{samp}' not in runlib2samp.tsv")
        ssets[setname] = samples
    ssets["all_samples"] = list(sorted(everything))

    # Don't sort these sample lists
    return ssets


def unit_normalise(resdict):
    ret = {}
    for res, val in resdict.items():
        if res == "mem_gb":
            res = "mem_mb"
            val = int(val) * 1024
        if res == "disk_gb":
            res = "disk_mb"
            val = int(val) * 1024
        if res == "runtime_hour":
            res = "runtime"
            val = int(val) * 60
        ret[res] = val
    return ret


def rule_resources(config, rule, **defaults):
    def resource(wildcards, attempt, value, maxvalue):
        return int(min(value * 2^(attempt-1), maxvalue))
    C = config.get("resources", {})
    maxes = unit_normalise(C.get("__max__", {}))
    global_defaults = unit_normalise(C.get("__default__", {}))
    rule_config = unit_normalise(C.get(rule, {}))
    
    values = {}
    values.update(global_defaults)
    values.update(unit_normalise(defaults))
    values.update(rule_config)
    values = unit_normalise(values)
    ret = {}

    if C.get("__DEBUG__", False):
        print(f"{rule}:")
    for res, val in values.items():
        maxval = maxes.get(res, inf)
        if val > maxval:
            val = maxval
        # Also broken TODO
        # if isinstance(val, str):
	#    # the logic below allows restarting with increased resources. If
	#    # the resource's value is string, you can't double it with each
	#    # attempt, so just return it as a constant.
	#    # this is used for things like cluster queues etc.
	#    ret[res] = val
	#    if C.get("__DEBUG__", False):
	#        print(f"  {res}: {val}")
	#    continue
        if C.get("__DEBUG__", False):
            print(f"  {res}: {val}  	# Max val = {maxval}")
        ret[res] = val # TODO this is broken: partial(resource, value=val, maxvalue=maxval)
    return ret


def postprocess_rules(workflow, config):
    R = config.get("resources", {})
    D = R.get("__default__", {
        "cores": 1,
        "disk_mb": 1000,
        "mem_mb": 1000,
        "runtime": 120,
        "localrule": 0,
    })
    for rule_obj in workflow.rules:
        n = rule_obj.name
        res = deepcopy(D)
        res.update(rule_obj.resources)
        res.update(R.get(n, {}))
        res = unit_normalise(res)
        for key, val in res.items():
            # Blacklist these
            if key not in ["localrule", "cores"]:
                rule_obj.resources[key] = val
        if "cores" in res:
            res["cores"] = int(res["cores"]) # TODO this is broken: float(res['cores']) * float(R.get("__fractional_cpu_scalar__", 1))
            rule_obj.resources["_cores"] = res["cores"]
        if res.get("localrule", 0):
            workflow._localrules.add(n)


def T(paths):
    if isinstance(paths, str):
        paths = [paths, ]
    opaths = []
    for path in paths:
        opaths.append(config["data_paths"].get("temp_prefix", "") + path)
    if len(opaths) == 1:
        return opaths[0]
    return opaths

def L(path):
    return P(path)


def R(paths, keep_local=False):
    from snakemake.common import parse_uri
    if isinstance(paths, str):
        paths = [paths, ]
    paths2 = []
    for path in paths:
        if isinstance(path, list):
            for p in path:
                paths2.apply(p)
        else:
            paths2.append(path)
    paths = paths2

    opaths = []
    if "REMOTE_PROVIDERS" not in config:
        config["REMOTE_PROVIDERS"] = {}
    for path in paths:
        uri = parse_uri(path)
        # This logic is very similar to snakemake.remotes.AUTO, except that
        # this actually works and doesn't wait 10 minutes for various remotes
        # to time out before failing to import them. TODO port this upstream to
        # Snakemake's AutoRemoteProvider.
        if uri.scheme.lower() in ["", "file"]:
            opaths.append(path)
        elif uri.scheme.lower() == "s3":
            from snakemake.remote.S3 import RemoteProvider
            if "S3" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["S3"] = RemoteProvider()
            S3 = config["REMOTE_PROVIDERS"]["S3"]
            opaths.append(S3.remote(path, keep_local=keep_local))
        elif uri.scheme.lower() == "gs":
            from snakemake.remote.GS import RemoteProvider
            if "GS" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["GS"] = RemoteProvider()
            GS = config["REMOTE_PROVIDERS"]["GS"]
            opaths.append(GS.remote(path, keep_local=keep_local))
        elif uri.scheme.lower() in ["ftp", "ftps"]:
            from snakemake.remote.FTP import RemoteProvider
            if "FTP" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["FTP"] = RemoteProvider()
            FTP = config["REMOTE_PROVIDERS"]["FTP"]
            opaths.append(FTP.remote(path, keep_local=keep_local))
        elif uri.scheme.lower() in ["http", "https"]:
            from snakemake.remote.HTTP import RemoteProvider
            if "HTTP" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["HTTP"] = RemoteProvider()
            HTTP = config["REMOTE_PROVIDERS"]["HTTP"]
            opaths.extend(HTTP.remote(path, keep_local=keep_local))
        elif uri.scheme.lower() in ["az", "azblob"]:
            from snakemake.remote.AzBlob import RemoteProvider
            if "AzBlob" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["AzBlob"] = RemoteProvider()
            AzBlob = config["REMOTE_PROVIDERS"]["AzBlob"]
            opaths.append(AzBlob.remote(path, keep_local=keep_local))
        elif uri.scheme.lower() in ["sftp", "ssh"]:
            from snakemake.remote.SFTP import RemoteProvider
            if "SFTP" not in config["REMOTE_PROVIDERS"]:
                config["REMOTE_PROVIDERS"]["SFTP"] = RemoteProvider()
            SFTP = config["REMOTE_PROVIDERS"]["SFTP"]
            opaths.append(SFTP.remote(path, keep_local=keep_local))
        else:
            raise ValueError(f"Unsupported scheme in remote URI: {path} = '{uri.scheme}'")
    if len(opaths) == 1:
        return opaths[0]
    return opaths

def P(paths, keep_local=False):
    if isinstance(paths, str) or isinstance(paths, pathlib.Path):
        paths = [paths, ]
    opaths = [R(config["data_paths"].get("persistent_prefix", "") + p, keep_local=keep_local) for p in paths]
    if len(opaths) == 1:
        return opaths[0]
    return opaths



shell.prefix = "set -exuo pipefail; ulimit -s $(ulimit -s -H); ulimit -n $(ulimit -n -H) ; "

wildcard_constraints:
    run="[^/~]+",
    lib="[^/~]+",
    aligner="[^/~]+",
    sample="[^/~]+",
    sampleset="[^/~]+",
    ref="[^/~]+",
    type="[^/~]+",

resource_scopes:
    runtime="local",

container: "docker://quay.io/condaforge/miniforge3"
